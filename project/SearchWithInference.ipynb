{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import gensim\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import ZODB, ZODB.FileStorage\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Current design is a simple 2 DB design - will not scale\n",
    "# for full multi doc indexing,\n",
    "# would need to breakdown datastores to faciliate concurrency, \n",
    "# and different document types and meta data needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ngram Datastore (Core index)\n",
    "storageNgram = ZODB.FileStorage.FileStorage('data/ngram.fs')\n",
    "dbNgram = ZODB.DB(storageNgram)\n",
    "connectionNgram = dbNgram.open()\n",
    "rootNgram = connectionNgram.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Document MetaData Datastore (Target datastore)\n",
    "storageDocMeta = ZODB.FileStorage.FileStorage('data/docmeta.fs')\n",
    "dbDocMeta = ZODB.DB(storageDocMeta)\n",
    "connectionDocMeta = dbDocMeta.open()\n",
    "rootDocMeta = connectionDocMeta.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpusDir = './corpus' # where to scan files (currently only processing text)\n",
    "ngramWidth = 3         # Width to produce ngrams -> wider = more data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize Text Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace html breaks with newline\n",
    "    norm_text = re.sub(r'<br *\\/*>', '\\n', norm_text)\n",
    "    # Replace non-AlphaNumeric|Newline with Space\n",
    "    norm_text = re.sub(r'[^\\w\\n]+', ' ', norm_text)\n",
    "    return norm_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./corpus/Missing.txt\n",
      "1 ./corpus/contents.txt\n",
      "2 ./corpus/Kindle-Book-List-20130830 - Unknown.txt\n",
      "3 ./corpus/rq3.txt\n",
      "4 ./corpus/CASTLE.txt\n",
      "5 ./corpus/i11.txt\n",
      "6 ./corpus/dd_half.txt\n",
      "7 ./corpus/dd_elven.txt\n",
      "8 ./corpus/text8.txt\n",
      "9 ./corpus/LICENSE-README.txt\n",
      "10 ./corpus/appendix.txt\n",
      "11 ./corpus/dd_gnome.txt\n",
      "12 ./corpus/dd_drow.txt\n",
      "13 ./corpus/dd_dwarf.txt\n"
     ]
    }
   ],
   "source": [
    "dictList = {}  # Key=ngram, data=[fileids]\n",
    "filesDict = {} # Key=FileID, data={fileDict}\n",
    "fileDict = {}  # file=fullpath, <lineid>=[ngrams]\n",
    "\n",
    "path = os.path.join(corpusDir, '*.txt')\n",
    "files = glob.glob(path)\n",
    "# print(files)\n",
    "\n",
    "#Generate Vectorization of ngrams and strip stop words \n",
    "vectorizer = CountVectorizer(ngram_range=(1, ngramWidth), stop_words='english')\n",
    "ngramAnalyzer = bigram_vectorizer.build_analyzer()\n",
    "\n",
    "# for each file, get a UID and parse\n",
    "for fileID, fileName in enumerate(files):\n",
    "    # Build a individual File Breakdown dictionary\n",
    "    fileDict = {}\n",
    "    fileDict['_file'] = fileName\n",
    "    print fileID, fileName\n",
    "    with open( fileName, mode = 'rU' ) as currFile:\n",
    "        # for each line get a UID and parse line\n",
    "        for lineID, line in enumerate(currFile):\n",
    "            #print lineID, normalize_text(line)\n",
    "\n",
    "            # store the lines vectorization for later analysis\n",
    "            lineNgrams = ngramAnalyzer( normalize_text(line) )\n",
    "            #print lineID, lineNgrams\n",
    "                        \n",
    "            # For each word/ngram add to master dictionary with FileID & In FileDict\n",
    "            for item in lineNgrams:\n",
    "                # First Record Ngram is in File, then record which lines have the Ngram\n",
    "                \n",
    "                # initialize item if not already in the master dictionory\n",
    "                if item not in dictList:\n",
    "                  dictList[item] = [fileID]\n",
    "                elif fileID not in dictList[item]:\n",
    "                    # if File isn't recorded as a viable match, then add to list\n",
    "                    dictList[item].append(fileID)\n",
    "\n",
    "                # initialize item if not already in fileDict\n",
    "                if item not in fileDict:\n",
    "                  fileDict[item] = [lineID]\n",
    "                elif lineID not in fileDict[item]:\n",
    "                    # if line isn't recorded as a viable match, then add to list\n",
    "                    fileDict[item].append(lineID)\n",
    "                \n",
    "\n",
    "    # store file's analysis in master file list\n",
    "    filesDict[fileID] = fileDict\n",
    "print dictList\n",
    "print '======'\n",
    "print filesDict\n",
    "# model = gensim.models.Word2Vec(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
